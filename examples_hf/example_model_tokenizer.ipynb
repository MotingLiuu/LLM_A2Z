{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53ce7d55",
   "metadata": {},
   "source": [
    "# 1. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46ac032b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "model.save_pretrained(\"../models/bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b62be609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa4647ac45874ca69dceac54b09214b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad9c21a5635d4f84b246b90f82909eee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\", cache_dir=\"../models/bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb720881",
   "metadata": {},
   "source": [
    "# 2. Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079f8cf7",
   "metadata": {},
   "source": [
    "## 2.1 load and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23812d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jim', 'Henson', 'was', 'a', 'puppeteer']\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = \"Jim Henson was a puppeteer\".split()\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8a189da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../models/bert-base-cased/tokenizer_config.json',\n",
       " '../models/bert-base-cased/special_tokens_map.json',\n",
       " '../models/bert-base-cased/vocab.txt',\n",
       " '../models/bert-base-cased/added_tokens.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "tokenizer.save_pretrained(\"../models/bert-base-cased/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d485a761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../models/bert-base-cased/tokenizer_config.json',\n",
       " '../models/bert-base-cased/special_tokens_map.json',\n",
       " '../models/bert-base-cased/vocab.txt',\n",
       " '../models/bert-base-cased/added_tokens.json',\n",
       " '../models/bert-base-cased/tokenizer.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "tokenizer.save_pretrained(\"../models/bert-base-cased/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04643bda",
   "metadata": {},
   "source": [
    "## 2.2 encode and decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac5bcb86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Using', 'a', 'Trans', '##former', 'network', 'is', 'simple']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\", cache_dir=\"../models/bert-base-cased\")\n",
    "\n",
    "sequence = \"Using a Transformer network is simple\"\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb533831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7993, 170, 13809, 23763, 2443, 1110, 3014]\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbe0e905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102]\n"
     ]
    }
   ],
   "source": [
    "sequence_ids = tokenizer.encode(sequence)\n",
    "print(sequence_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b4090c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  7993,   170, 13809, 23763,  2443,  1110,  3014,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = tokenizer(\"Using a Transformer network is simple\", return_tensors=\"pt\")\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a04fe1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] Using a Transformer network is simple [SEP]\n",
      "Using a Transformer network is simple\n"
     ]
    }
   ],
   "source": [
    "decoded_string = tokenizer.decode(sequence_ids)\n",
    "print(decoded_string)\n",
    "decoded_ids = tokenizer.decode(ids)\n",
    "print(decoded_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b39c1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs:\n",
      " tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
      "          2026,  2878,  2166,  1012]])\n",
      "Output:\n",
      " SequenceClassifierOutput(loss=None, logits=tensor([[-2.7276,  2.8789]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Logits:\n",
      " tensor([[-2.7276,  2.8789]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "# input_ids = torch.tensor(ids), This line will fail.\n",
    "input_ids = torch.tensor([ids])\n",
    "print(\"Input IDs:\\n\", input_ids)\n",
    "\n",
    "output = model(input_ids)\n",
    "print(\"Output:\\n\", output)\n",
    "print(\"Logits:\\n\", output.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a3e7c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Inputs:\n",
      " {'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "Inputs Keys:\n",
      " dict_keys(['input_ids', 'attention_mask'])\n",
      "\n",
      "Input IDs:\n",
      " tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102]])\n",
      "\n",
      "Logits:\n",
      " tensor([[-1.5607,  1.6123]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "tokenized_inputs = tokenizer(sequence, return_tensors=\"pt\")\n",
    "print(\"Tokenized Inputs:\\n\", tokenized_inputs)\n",
    "print(\"Inputs Keys:\\n\", tokenized_inputs.keys())\n",
    "print(\"\\nInput IDs:\\n\", tokenized_inputs[\"input_ids\"])\n",
    "\n",
    "output = model(**tokenized_inputs)\n",
    "print(\"\\nLogits:\\n\", output.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b647ba27",
   "metadata": {},
   "source": [
    "## 2.3 padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6970bd71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 1.5694, -1.3895],\n",
      "        [ 1.3374, -1.2163]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence1_ids = [[200, 200, 200]]\n",
    "sequence2_ids = [[200, 200]]\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "\n",
    "print(model(torch.tensor(sequence1_ids)).logits)\n",
    "print(model(torch.tensor(sequence2_ids)).logits)\n",
    "print(model(torch.tensor(batched_ids)).logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a462550a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 1.5694, -1.3895],\n",
      "        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence1_ids = [[200, 200, 200]]\n",
    "sequence2_ids = [[200, 200]]\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "batched_attention_masks = [\n",
    "    [1, 1, 1],\n",
    "    [1, 1, 0],\n",
    "]\n",
    "\n",
    "print(model(torch.tensor(sequence1_ids)).logits)\n",
    "print(model(torch.tensor(sequence2_ids)).logits)\n",
    "outputs = model(\n",
    "    torch.tensor(batched_ids), \n",
    "    attention_mask=torch.tensor(batched_attention_masks))\n",
    "print(outputs.logits)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
