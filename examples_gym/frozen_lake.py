import gymnasium as gym
import numpy as np
import time

# --- 1. åˆå§‹åŒ–ç¯å¢ƒ ---
# is_slippery=True è¡¨ç¤ºç¯å¢ƒæ˜¯éšæœºçš„ï¼ˆåœ¨å†°ä¸Šèµ°å¯èƒ½ä¼šæ»‘åˆ°å…¶ä»–æ ¼å­ï¼‰
# è¿™æ˜¯ FrozenLake çš„æ ‡å‡†æ¨¡å¼ï¼Œæ›´èƒ½ä½“ç°ç­–ç•¥è¿­ä»£çš„ä»·å€¼ã€‚
env = gym.make("FrozenLake-v1", render_mode="human", is_slippery=False)

# è·å–çŠ¶æ€å’ŒåŠ¨ä½œçš„æ•°é‡
num_states = env.observation_space.n
num_actions = env.action_space.n

print(f"çŠ¶æ€ç©ºé—´æ•°é‡ (Number of states): {num_states}")
print(f"åŠ¨ä½œç©ºé—´æ•°é‡ (Number of actions): {num_actions}")

def policy_iteration(env, gamma=0.99, theta=1e-8):
    """
    ç­–ç•¥è¿­ä»£ç®—æ³•å®ç°

    Args:
        env: Gymnasium ç¯å¢ƒ.
        gamma (float): æŠ˜æ‰£å› å­.
        theta (float): ç­–ç•¥è¯„ä¼°çš„æ”¶æ•›é˜ˆå€¼.

    Returns:
        tuple: (æœ€ä¼˜ç­–ç•¥, æœ€ä¼˜ä»·å€¼å‡½æ•°).
    """
    # --- ç®—æ³•åˆå§‹åŒ– ---
    policy = np.random.randint(num_actions, size=num_states)
    V = np.zeros(num_states)

    iteration = 0
    while True:
        iteration += 1
        print(f"\n--- è¿­ä»£è½®æ¬¡: {iteration} ---")
        
        # --- 2. ç­–ç•¥è¯„ä¼° (Policy Evaluation) ---
        print("1. æ­£åœ¨è¿›è¡Œç­–ç•¥è¯„ä¼°...")
        while True:
            delta = 0
            for s in range(num_states):
                v_old = V[s]
                action = policy[s]
                # ã€ä¿®æ­£ã€‘ä½¿ç”¨ env.unwrapped.P è®¿é—®è½¬ç§»æ¨¡å‹
                V[s] = sum([prob * (reward + gamma * V[next_state])
                            for prob, next_state, reward, _ in env.unwrapped.P[s][action]])
                delta = max(delta, abs(v_old - V[s]))
            
            if delta < theta:
                print("   ç­–ç•¥è¯„ä¼°å®Œæˆï¼Œä»·å€¼å‡½æ•°å·²æ”¶æ•›ã€‚")
                break
        
        # --- 3. ç­–ç•¥æ”¹è¿› (Policy Improvement) ---
        print("2. æ­£åœ¨è¿›è¡Œç­–ç•¥æ”¹è¿›...")
        policy_stable = True
        for s in range(num_states):
            old_action = policy[s]
            
            action_values = np.zeros(num_actions)
            for a in range(num_actions):
                # ã€ä¿®æ­£ã€‘ä½¿ç”¨ env.unwrapped.P è®¿é—®è½¬ç§»æ¨¡å‹
                action_values[a] = sum([prob * (reward + gamma * V[next_state])
                                        for prob, next_state, reward, _ in env.unwrapped.P[s][a]])
            
            best_action = np.argmax(action_values)
            policy[s] = best_action
            
            if old_action != best_action:
                policy_stable = False

        if policy_stable:
            print("   ç­–ç•¥æ”¹è¿›å®Œæˆï¼Œç­–ç•¥å·²ç¨³å®šã€‚æ‰¾åˆ°æœ€ä¼˜ç­–ç•¥ï¼")
            break
            
    return policy, V

# --- è®­ç»ƒ Agent ---
print("å¼€å§‹ä½¿ç”¨ç­–ç•¥è¿­ä»£ç®—æ³•è¿›è¡Œè®­ç»ƒ...")
optimal_policy, optimal_value_function = policy_iteration(env)

print("\n--- è®­ç»ƒå®Œæˆ ---")
print("æœ€ä¼˜ç­–ç•¥ (0:å·¦, 1:ä¸‹, 2:å³, 3:ä¸Š):")
print(optimal_policy.reshape(4, 4))
print("\næœ€ä¼˜ä»·å€¼å‡½æ•°:")
print(optimal_value_function.reshape(4, 4))


# --- 4. ä½¿ç”¨è®­ç»ƒå¥½çš„ç­–ç•¥ä¸ç¯å¢ƒäº¤äº’å¹¶æ¸²æŸ“ ---
print("\n--- å¼€å§‹ä½¿ç”¨æœ€ä¼˜ç­–ç•¥è¿›è¡Œæ¼”ç¤º (å…± 3 ä¸ªå›åˆ) ---")
for episode in range(3):
    state, info = env.reset()
    terminated = False
    truncated = False
    print(f"\n--- ç¬¬ {episode + 1} å›åˆ ---")
    time.sleep(1) # æš‚åœ1ç§’ï¼Œæ–¹ä¾¿è§‚å¯Ÿ
    
    while not terminated and not truncated:
        # ä¸éœ€è¦å†æ¢ç´¢ï¼Œç›´æ¥ä½¿ç”¨æœ€ä¼˜ç­–ç•¥
        action = optimal_policy[state]
        
        # ä¸ç¯å¢ƒäº¤äº’
        state, reward, terminated, truncated, info = env.step(action)
        
        # env.render() ä¼šåœ¨ step åè‡ªåŠ¨è°ƒç”¨ (å› ä¸º render_mode="human")
        time.sleep(0.3) # æ¯ä¸€æ­¥ç¨ä½œæš‚åœï¼Œæ–¹ä¾¿è§‚å¯Ÿ
        
    if terminated and reward == 1.0:
        print("æˆåŠŸåˆ°è¾¾ç»ˆç‚¹ï¼ğŸ‰")
    else:
        print("æ‰è¿›å†°çªŸæˆ–è¶…æ—¶ã€‚ğŸ˜­")

# --- 5. å…³é—­ç¯å¢ƒ ---
env.close()